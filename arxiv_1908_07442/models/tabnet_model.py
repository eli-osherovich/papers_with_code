"""TabNet implementation in modern TF

  Reference:
      https://arxiv.org/abs/1908.07442
"""

from typing import Any, Optional

from absl import flags
import gin
import tensorflow as tf
import tensorflow_addons as tfa

from . import tasks
from ...common.types import Activation

FLAGS = flags.FLAGS

# Inputs/outputs of each step:
_PREPROC_DATA = "preproc_data"
_MASK = "mask"
_COMP_AGG_MASK = "comp_agg_mask"
_DECISION = "decision"

# Internal features. These are generated by the Feature Transformer model.
_MASK_FEATURES = "mask_features"
_DECISION_FEATURES = "decision_features"


def glu(x: tf.Tensor) -> tf.Tensor:
  """Generalized linear unit nonlinear activation."""
  half_dim = x.shape[-1] // 2
  return x[:, :half_dim] * tf.nn.sigmoid(x[:, half_dim:])


class TabNetBlock(tf.keras.Model):
  """Basic building block of TabNet.

  The basic block of the TabNet consists of the three layers:
  Dense -> BatchNorm -> Activation.
  An existing dense layer can be passed as an argument. This mechanism allows
  one to share a layer among several blocks. If not provided, a new one
  (non-shared) will be automatically created using `dense_args`
  arguments.

  Args:
      shared_dense (tf.keras.layers.Dense, optional): Shared dense layer. If not
        provided, the block with generate it using `dense_args` in this case,
        the layer will *not* be shared. Defaults to None.
      activation (Activation): Activation used at the block's output. Defaults
        to 'relu'.
      dense_args (dict, optional): Arguments for the dense layer. Used only if
        an external (shared) denslayer was not provided.
      batch_norm_args (dict, optional): Arguments to the BatchNorm layer.
      kwargs (dict, optional): Arguments passed to the parent class.
  """

  def __init__(
    self,
    *,
    shared_dense: Optional[tf.keras.layers.Dense] = None,
    activation: Activation = glu,
    skip_conn: bool = True,
    scale: float = 1.0,
    dense_args: dict[str, Any] = {},
    batch_norm_args: dict[str, Any] = {},
    **kwargs
  ) -> None:
    super().__init__(**kwargs)
    self.dense = shared_dense or tf.keras.layers.Dense(**dense_args)
    self.batch_norm = tf.keras.layers.BatchNormalization(**batch_norm_args)
    self.activation = tf.keras.layers.Activation(activation)
    self.skip_conn = skip_conn
    self.scale = scale
    # self.model will be created in build()

  def build(self, input_shape: tf.TensorShape) -> None:
    batch_size, *shape = input_shape
    x = tf.keras.Input(shape=shape, batch_size=batch_size)
    f = self.dense(x)
    f = self.batch_norm(f)
    f = self.activation(f)

    # All but the first blocks use skip connection.
    if self.skip_conn:
      f = (f + x) * self.scale

    self.model = tf.keras.Model(x, f)

  def call(self, inputs: tf.Tensor, **kwargs) -> tf.Tensor:
    return self.model(inputs, **kwargs)


class TabNetStep(tf.keras.Model):
  """FeatureTransformer of TabNet.


  Args:
      shared_dense1 (tf.keras.layers.Dense): First shared layer in every step.
      shared_dense2 (tf.keras.layers.Dense): Second shared layer in every step.
      scale (float, optional): Scaling coefficient used between the blocks.
        Defaults to sqrt(0.5),
      dense_args (dict, optional): Arguments for the non-shared dense layers.
      bn_args (dict, optional): Arguments for the BatchNorm layers.
  """

  def __init__(
    self,
    *,
    shared_dense1: tf.keras.layers.Dense,
    shared_dense2: tf.keras.layers.Dense,
    mask_features_dim: int,
    decision_features_dim: int,
    relaxation_factor: float = 1.0,
    scale: float = 0.5**0.5,
    block_activation: Activation = glu,
    dense_args: dict = {},
    batch_norm_args: dict = {},
    **kwargs
  ) -> None:
    super().__init__(**kwargs)
    self.shared_dense1 = shared_dense1
    self.shared_dense2 = shared_dense2
    self.mask_features_dim = mask_features_dim
    self.decision_features_dim = decision_features_dim
    self.relaxation_factor = relaxation_factor
    self.scale = scale
    self.block_activation = block_activation
    self.dense_args = dense_args
    self.batch_norm_args = batch_norm_args
    # self.model will be created in build()

  def build(self, input_shape):
    self.feature_transformer_model = self._build_feature_transformer_model(
      input_shape
    )

    # Infer intermediate shapes from the model which produces them.
    mask_features_shape = self.feature_transformer_model.output_shape[
      _MASK_FEATURES]
    decision_features_shape = self.feature_transformer_model.output_shape[
      _DECISION_FEATURES]

    # Subsequent models will extract the necessary shapes by keys.
    all_shapes = input_shape | {
      _MASK_FEATURES: mask_features_shape,
      _DECISION_FEATURES: decision_features_shape
    }

    self.decision_model = self._build_decision_model(all_shapes)
    self.mask_model = self._build_mask_model(all_shapes)

  def _build_feature_transformer_model(self, input_shape):
    # Feature transformer receives (masked) preprocessed data.
    batch_size, *shape = input_shape[_PREPROC_DATA]

    b1 = TabNetBlock(
      shared_dense=self.shared_dense1,
      activation=self.block_activation,
      skip_conn=False,
      scale=1.0,
      batch_norm_args=self.batch_norm_args
    )
    b2 = TabNetBlock(
      shared_dense=self.shared_dense2,
      activation=self.block_activation,
      skip_conn=True,
      scale=self.scale,
      batch_norm_args=self.batch_norm_args
    )
    b3 = TabNetBlock(
      activation=self.block_activation,
      skip_conn=True,
      scale=self.scale,
      dense_args=self.dense_args,
      batch_norm_args=self.batch_norm_args
    )
    b4 = TabNetBlock(
      activation=self.block_activation,
      skip_conn=True,
      scale=self.scale,
      dense_args=self.dense_args,
      batch_norm_args=self.batch_norm_args
    )
    x = tf.keras.Input(shape=shape, batch_size=batch_size)
    f = b1(x)
    f = b2(f)
    f = b3(f)
    f = b4(f)

    return tf.keras.Model(
      x,
      outputs={
        _MASK_FEATURES: f[:, :self.mask_features_dim],
        _DECISION_FEATURES: f[:, -self.decision_features_dim:]
      }
    )

  def _build_mask_model(self, input_shape):
    # Both mask and complimentary mask have the same shape.
    mask_shape = input_shape[_COMP_AGG_MASK]

    batch_size, input_dim = mask_shape
    inputs = {
      _MASK_FEATURES:
        tf.keras.Input(
          shape=(self.mask_features_dim,),
          batch_size=batch_size,
          name=_MASK_FEATURES
        ),
      _COMP_AGG_MASK:
        tf.keras.Input(
          shape=(input_dim,), batch_size=batch_size, name=_COMP_AGG_MASK
        )
    }

    m = tf.keras.layers.Dense(input_dim)(inputs[_MASK_FEATURES])
    m = tf.keras.layers.BatchNormalization()(m)
    m *= inputs[_COMP_AGG_MASK]
    mask = tfa.activations.sparsemax(m)
    complimentary_agg_mask_out = inputs[_COMP_AGG_MASK] * (
      self.relaxation_factor - mask
    )
    return tf.keras.Model(
      inputs=inputs,
      outputs={
        _MASK: mask,
        _COMP_AGG_MASK: complimentary_agg_mask_out
      },
    )

  def _build_decision_model(self, input_shape):
    batch_size, shape = input_shape[_DECISION_FEATURES]
    x = tf.keras.Input(
      shape=shape, batch_size=batch_size, name=_DECISION_FEATURES
    )
    decision = tf.keras.activations.relu(x)
    return tf.keras.Model(x, {_DECISION: decision})

  def call(self, inputs, **kwargs):
    masked_x = inputs[_PREPROC_DATA] * inputs[_MASK]
    ft_res = self.feature_transformer_model(masked_x, **kwargs)
    mask_res = self.mask_model(inputs | ft_res)
    decision = self.decision_model(inputs | ft_res)
    decision[_DECISION] += inputs[_DECISION]
    return inputs | mask_res | decision


class TabNet(tf.keras.Model):

  def __init__(
    self,
    *,
    n_steps: int = 4,
    shared_dense_args: dict[str, Any],
    mask_features_dim: int,
    decision_features_dim: int,
    relaxation_factor: float = 1.0,
    scale: float = 0.5**0.5,
    block_activation: Activation = glu,
    dense_args: dict[str, Any] = {},
    batch_norm_args: dict[str, Any] = {},
    **kwargs
  ) -> None:
    super().__init__(**kwargs)
    shared1 = tf.keras.layers.Dense(**shared_dense_args)
    shared2 = tf.keras.layers.Dense(**shared_dense_args)
    self.steps = [
      TabNetStep(
        shared_dense1=shared1,
        shared_dense2=shared2,
        mask_features_dim=mask_features_dim,
        decision_features_dim=decision_features_dim,
        relaxation_factor=relaxation_factor,
        scale=scale,
        block_activation=block_activation,
        dense_args=dense_args,
        batch_norm_args=batch_norm_args,
      ) for _ in range(n_steps)
    ]
    self.mask_features_dim = mask_features_dim
    self.decision_features_dim = decision_features_dim
    self.relaxation_factor = relaxation_factor
    self.scale = scale
    self.block_activation = block_activation
    self.dense_args = dense_args
    self.batch_norm_args = batch_norm_args

  def build(self, input_shape):
    batch_size, _ = input_shape
    self.preproc_model = self._build_preproc_model(input_shape)
    preproc_data_dim = self.preproc_model.output_shape[1]

    # Initial (zero-step) values.
    self.mask0 = tf.ones((batch_size, preproc_data_dim))
    self.comp_agg_mask0 = tf.ones_like(self.mask0)
    self.decision0 = 0.0  # will broadcast to the right shape.

  def _build_preproc_model(self, input_shape):
    batch_size, *shape = input_shape
    x = tf.keras.Input(shape=shape, batch_size=batch_size)
    y = tf.keras.layers.BatchNormalization(**self.batch_norm_args)(x)
    return tf.keras.Model(x, y)

  def call(self, inputs, **kwargs):
    preproc_data = self.preproc_model(inputs, **kwargs)
    # All steps use the same structure for input and output.
    # Note the the decision is the aggregated value (sum) across all the
    # previous steps.
    step_io = {
      _PREPROC_DATA: preproc_data,
      _MASK: self.mask0,
      _DECISION: self.decision0,
      _COMP_AGG_MASK: self.comp_agg_mask0,
    }
    for step in self.steps:
      step_io = step(step_io, **kwargs)
    return step_io


class TabNetWrapper(TabNet):
  """TabNetWrapper with a single output.

  A simple wrapper that extracts only one output of the tabnet: decision.
  It is need to let one use TabNet in a Sequential model. Which, in turn,
  provides the convenience of letting one not specify input shape.
  """

  def __init__(self, *args, **kwargs) -> None:
    super().__init__(*args, **kwargs)

  def call(self, *args, **kwargs):
    res = super().call(*args, **kwargs)
    return res[_DECISION]


@gin.configurable
def get_model(**kwargs) -> tf.keras.Model:
  if FLAGS.task == tasks.TASK.REGRESSION:
    return get_regression_model(**kwargs)
  elif FLAGS.task == tasks.TASK.BINARY:
    return get_binary_model(**kwargs)
  elif FLAGS.task == tasks.TASK.MULTICLASS:
    return get_multiclass_model(**kwargs)
  else:
    raise RuntimeError(f"Wrong task type: {FLAGS.type}")


@gin.configurable
def get_regression_model(**kwargs) -> tf.keras.Model:
  # Add a regression head to the main trunk.
  model = tf.keras.Sequential([
    TabNet(**kwargs),
    tf.keras.layers.Dense(1),
  ])
  model.compile(loss="mse", optimizer="adam", metrics=["RootMeanSquaredError"])
  return model


@gin.configurable
def get_binary_model(**kwargs) -> tf.keras.Model:
  # Add a binary classification head to the main trunk.
  model = tf.keras.Sequential([
    TabNet(**kwargs),
    tf.keras.layers.Dense(1, activation="sigmoid"),
  ])
  model.compile(loss="bce", optimizer="adam", metrics=["accuracy"])
  return model


@gin.configurable
def get_multiclass_model(num_classes: int = 3, **kwargs) -> tf.keras.Model:
  # Add a binary classification head to the main trunk.
  model = tf.keras.Sequential([
    TabNetWrapper(**kwargs),
    tf.keras.layers.Dense(num_classes, activation="softmax"),
  ])

  model.compile(
    loss="SparseCategoricalCrossentropy",
    optimizer="adam",
    metrics=["accuracy"]
  )
  return model
